{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Indexing Collections of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Term Document Incidence Matrix\n",
    "-----\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>12-11-1928</th>\n",
    "      <th>04-04-1946</th>\n",
    "      <th>03-11-1983</th>\n",
    "      <th>19-01-1999</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Einstein</th>\n",
    "      <td> 1</td>\n",
    "      <td> 1</td>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Hubble</th>\n",
    "      <td> 1</td>\n",
    "      <td> 1</td>\n",
    "      <td> 1</td>\n",
    "      <td> 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Fermi</th>\n",
    "      <td> 1</td>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Winfrey</th>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "      <td> 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Dylan</th>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "      <td> 1</td>\n",
    "      <td> 1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "This table is called a *Term Document incidence matrix* where for each term (rows) we write down a 1 if a particular document (columns) contains that term and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each term is represented by an incidence vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Term Document Incidence Matrix\n",
    "-----\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>12-11-1928</th>\n",
    "      <th>04-04-1946</th>\n",
    "      <th>03-11-1983</th>\n",
    "      <th>19-01-1999</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Einstein</th>\n",
    "      <td> 1</td>\n",
    "      <td> 1</td>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Hubble</th>\n",
    "      <td> 1</td>\n",
    "      <td> 1</td>\n",
    "      <td> 1</td>\n",
    "      <td> 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Fermi</th>\n",
    "      <td> 1</td>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Winfrey</th>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "      <td> 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Dylan</th>\n",
    "      <td> 0</td>\n",
    "      <td> 0</td>\n",
    "      <td> 1</td>\n",
    "      <td> 1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "If we want to extract all documents that mention Albert Einstein, we can use the binary vector `1100` of Einstein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To find all documents containing both Einstein and Hubble we take the complement of the vectors of Einstein (1100) and Hubble (1110): `1100 AND 1110 = 1100`   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sidebar: Truth Table\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://onegoodmove.org/fallacy/images/and.gif\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True and True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True and False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False and False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True and True and True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True and True and False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for Understanding\n",
    "-------\n",
    "\n",
    "Implement the function `AND` that takes as argument two incidence vectors (represented a binary lists in Python) and returns the complement of the two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def AND(vector_a, vector_b):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assert (AND([1, 1, 0, 0], \n",
    "            [1, 1, 1, 0]) == \n",
    "            [1, 1, 0, 0])\n",
    "\n",
    "assert (AND([1, 0, 0, 1, 0, 0, 1], \n",
    "            [1, 1, 1, 0, 1, 0, 1]) == \n",
    "            [1, 0, 0, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(True and True), (True and False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def AND(vector_a, vector_b):\n",
    "    return [1 if (a and b) else 0 \n",
    "                for a, b in zip(vector_a, vector_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rewrite the function `AND` to allow it to take an arbitary number of incidence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assert (AND([1, 1, 0, 0], \n",
    "            [1, 1, 1, 0], \n",
    "            [1, 0, 0, 0]) == \n",
    "            [1, 0, 0, 0])\n",
    "assert AND([1, 1, 1, 0, 1], [1, 0, 0, 1, 0], [0, 1, 1, 0, 1]) == [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def AND(*vectors):\n",
    "    return [1 if all(row) else 0 for row in zip(*vectors)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def NOT(vector):\n",
    "    \"Takes an incidence vector and returns a representation that can be used in combination with AND queries.\"\n",
    "    return [1 if not value else 0 for value in vector]\n",
    "\n",
    "assert AND([1, 1, 0, 0], [1, 1, 1, 0], NOT([1, 0, 0, 0])) == [0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Limitations of Binary Representation\n",
    "------\n",
    "\n",
    "The binary representation allows us to efficiently search for documents containing particular terms of a search query. \n",
    "\n",
    "However, consider a document collection of 1 million documents where each document is about a thousand words long. O n average such a collection contains approximately 500k unique terms. This means that if we try to build an incidence matrix, we would have to construct a matrix containing 500,000 x 1,000,000 = half a trillion 0's and 1's. Such a large matrix cannot be stored on single machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Inverted Index\n",
    "-----\n",
    "\n",
    "The *Inverted Index* is *the* key data structure in modern Information Retrieval. \n",
    "\n",
    "An inverted index is a structure in which for each unique term $t$ in our collection, we store a list of all document ID's that contain $t$. If we represent our small collection of newspapers in this way, it looks like this:\n",
    "\n",
    "`Einstein: [12-11-1928, 04-04-1946]`   \n",
    "`Hubble:   [12-11-1928, 04-04-1946, 03-11-1983]`   \n",
    "`Fermi:    [12-11-1928]`   \n",
    "`Winfrey:  [19-01-1999]`   \n",
    "`Dylan:    [03-11-1983, 19-01-1999]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Implement an IR system\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "def tokenize(text, lowercase=True):\n",
    "    text = text.lower() if lowercase else text\n",
    "    for match in re.finditer(r\"\\w+(\\.?\\w+)*\", text):\n",
    "        yield match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class IRSystem:\n",
    "    \"A very simple Information Retrieval System.\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"Initialize an IR Sytem.\"\n",
    "        self.tdf = defaultdict(set)\n",
    "        self.doc_ids = []\n",
    "                \n",
    "    def index_document(self, doc_id, words):\n",
    "        \"Add a new unindexed document to the system.\"\n",
    "        self.doc_ids.append(doc_id)\n",
    "        for word in words:\n",
    "            self.tdf[word].add(doc_id)\n",
    "        \n",
    "    def index_collection(self, filenames):\n",
    "        \"Index a collection of documents.\"\n",
    "        for filename in filenames:\n",
    "            self.index_document(os.path.basename(filename), \n",
    "                                tokenize(open(filename).read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "s = IRSystem()\n",
    "path = 'lab_information_retrieval/data/RiderHaggard/raw/*.txt'\n",
    "s.index_collection(glob.glob(path))\n",
    "\n",
    "# Tests\n",
    "assert 'The Ghost Kings 8184.txt' in s.tdf['master']\n",
    "assert 'Cleopatra 2769.txt' in s.tdf['children']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Query the Index\n",
    "\n",
    "Now that we have an efficient, sparse data structure to represent our collection, how can we use that stucture to efficiently process user queries? Our index is represented as a Python dictionary which allows us to query the index for single terms, using\n",
    "\n",
    "    s = IRSystem()\n",
    "    s.tdf[term]\n",
    "\n",
    "which will return a set of all documents in which that term occurs. But how do we search for documents that contain two or more terms? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Python's data structure `set` defines a convenient method called `intersection` with which we can extract all items common to two or more sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c', 'a'}\n"
     ]
    }
   ],
   "source": [
    "things_1 = {'a', 'b', 'c', 'd'}\n",
    "things_2 = {'c', 'a', 'e', 'f'}\n",
    "print(things_1.intersection(things_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can make use of this method to implement the method `query` which takes as argument an arbitrary number of query terms and returns all documents in which those terms occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "\n",
    "    def query(self, *terms):\n",
    "        \"Query the system for documents in which all terms occur.\"\n",
    "        return set.intersection(*map(self.tdf.get, terms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The Ancient Allan 5746.txt', \"Montezuma's Daughter 1848.txt\", 'The Wizard 2893.txt', 'Heu-Heu (1924) 0200191.txt', 'Marie An Episode in The Life of the late Allan Quatermain 1690.txt', 'Benita, an African romance 2761.txt', 'Colonel Quaritch, V.C. A Tale of Country Life 11882.txt', 'The Brethren 2762.txt', 'Regeneration 13434.txt', \"King Solomon's Mines 2166.txt\", 'Ayesha, the Return of She 5228.txt', \"Maiwa's Revenge 2713.txt\", 'Finished 1724.txt', 'Allan and the Ice Gods (1927) 0200201.txt', 'Fair Margaret 9780.txt', \"The World's Desire 2763.txt\", 'Pearl-Maiden 5175.txt', 'She 3155.txt', 'The Ghost Kings 8184.txt', 'A Yellow God: an Idol of Africa 2857.txt', \"The Wanderer's Necklace 3097.txt\", 'When the World Shook; being an account of the great adventure of Bastin, Bickley and Arbuthnot 1368.txt', 'Beatrice 3096.txt', 'Queen of the Dawn (1925) 0200381.txt', 'Moon of Israel 2856.txt', 'Red Eve 3094.txt', 'The Tale of Three Lions 2729.txt', \"Wisdom's Daughter (1923) 0200181.txt\", 'Eric Brighteyes 2721.txt', 'Cleopatra 2769.txt', 'Lysbeth, a Tale of the Dutch 5754.txt', 'The Ivory Child 2841.txt', 'Cetywayo and his White Neighbours Remarks on Recent Events in Zululand, Natal, and the Transvaal 8667.txt', 'Love Eternal 3709.txt', 'Stories by English Authors: Africa (Selected by Scribners) 1980.txt', 'Allan Quatermain 711.txt', 'She and Allan 5745.txt', 'A Winter Pilgrimage (1901) 0600121.txt', 'Elissa 2855.txt', \"Mr. Meeson's Will 11913.txt\", 'Morning Star 2722.txt', 'Dawn 10892.txt', 'The People of the Mist 6769.txt', 'Jess 5898.txt', \"Queen Sheba's Ring 2602.txt\", 'The Lady of Blossholme 3813.txt', 'The Mahatma and the Hare 2764.txt', 'Stella Fregelius 6051.txt', 'The Virgin of the Sun 3153.txt', \"Allan's Wife 2727.txt\", \"The Witch's Head (1884) 0500791.txt\", 'Child of Storm 1711.txt', 'Doctor Therne 5764.txt', 'Nada the Lily 1207.txt', 'Smith and the Pharaohs, and other Tales 6073.txt', 'Allan and the Holy Flower 5174.txt', 'Swallow: a tale of the great trek 4074.txt'}\n"
     ]
    }
   ],
   "source": [
    "s2 = IRSystem()\n",
    "s2.index_collection(glob.glob(path))\n",
    "\n",
    "print( s2.query(\"master\", \"children\"))\n",
    "# Tests\n",
    "assert 'Beatrice 3096.txt' in s2.query(\"master\", \"children\")\n",
    "assert 'Fair Margaret 9780.txt' in s2.query(\"eye\", \"father\", \"work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary thus far\n",
    "-----\n",
    "\n",
    "Our Information Retrieval system starts to look quite good. We have written functions to tokenize documents, index \n",
    "documents and query the index for documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However....\n",
    "-------\n",
    "\n",
    "In the query method above, we made the simplifying assumption that as long as two documents contain a particular search term, they are equally relevant. However, our intuition says that documents that contain more instances of a particular term are more relevant than documents with less instances. To account for this intuition we need a way to score and sort our search results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with a very naive and simple scoring function: document frequency. This scoring function simply sums the document frequencies of each search term in a particular document:\n",
    "\n",
    "$$\\textrm{score}(q_1, q_2, \\ldots, q_n) = \\sum^n_{i=1} df(q_i)$$\n",
    "\n",
    "where $n$ is the number of search terms and $df$ the document frequency of term $q_i$ in a document.\n",
    "\n",
    "To compute this formula we need to obtain the frequency of each word in each document. The method `index_document` seems an appropriate place to extract these frequencies. For each term we store how often that term occurs in each document. Note that in the previous implementation of `index_document`, the variable `tdf` is represented by a dictionary with sets of document ID's as values. We will change this data structure to a structure that allows us to store the document frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class IRSystem:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"Initialize an IR Sytem.\"\n",
    "        self.tdf = defaultdict(Counter) # changed!\n",
    "        self.doc_ids = []\n",
    "                \n",
    "    def index_document(self, doc_id, words):\n",
    "        \"Add a new unindexed document to the system.\"\n",
    "        self.doc_ids.append(doc_id)\n",
    "        for word in words:\n",
    "            self.tdf[word][doc_id] += 1 # changed!\n",
    "\n",
    "    def index_collection(self, filenames):\n",
    "        \"Index a collection of documents.\"\n",
    "        for filename in filenames:\n",
    "            self.index_document(os.path.basename(filename), \n",
    "                                tokenize(open(filename).read()))            \n",
    "            \n",
    "    def query(self, *terms):\n",
    "        \"Query the system for documents in which all terms occur.\"\n",
    "        return set.intersection(*map(self.tdf.get, terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "s = IRSystem()\n",
    "s.index_collection(glob.glob(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Ancient Allan 5746.txt', 161),\n",
       " ('Red Eve 3094.txt', 125),\n",
       " ('Lysbeth, a Tale of the Dutch 5754.txt', 116),\n",
       " ('The Virgin of the Sun 3153.txt', 90),\n",
       " ('The Brethren 2762.txt', 65),\n",
       " ('Fair Margaret 9780.txt', 61),\n",
       " ('The Lady of Blossholme 3813.txt', 60),\n",
       " (\"Wisdom's Daughter (1923) 0200181.txt\", 58),\n",
       " ('Pearl-Maiden 5175.txt', 57),\n",
       " ('Finished 1724.txt', 37)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.tdf['master'].most_common(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The class `Counter` has a method called `most_common`. It returns the $n$ most common items in a `Counter` object. We have a data structure that stores the information about the frequency of words in documents. The next step will be to to adapt our query function in such a way that it returns a ranked list of documents where each document is sorted on the basis of the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    \n",
    "    def score(self, doc_id, *terms):\n",
    "        \"Score a document for a particular query using the sum of the term frequencies.\"\n",
    "        # insert your code here\n",
    "        return sum(self.tdf[term][doc_id] for term in terms)\n",
    "            \n",
    "    def query(self, *terms, n=10):\n",
    "        \"\"\"Query the system for documents in which all terms occur. Returns\n",
    "        the top n matching documents.\"\"\"\n",
    "        scores = {doc_id: self.score(doc_id, *terms) for doc_id in self.doc_ids}\n",
    "        return sorted(scores, key=scores.get, reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "s = IRSystem()\n",
    "s.index_collection(glob.glob(path))\n",
    "\n",
    "assert s.query(\"master\")[0] == 'The Ancient Allan 5746.txt'\n",
    "assert s.query(\"egg\", \"shell\")[0] == 'Dawn 10892.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Why this way of scoring and sorting our documents is probably not such a good idea, after all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Keyword spamming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Okapi BM25 - Another version of tf-idf\n",
    "-----\n",
    "\n",
    "Our scoring method contains multiple flaws. Most worrying is that it does not control for the lengths of our \n",
    "documents. Needless to say, this greatly influences our final lists. We need to think harder about what makes it that a search term is more or less relevant for a particular document.\n",
    "\n",
    "The frequency with which terms occur in a document functions as an important cue to the importance of a document. The document is even more important when it contains many examples of this search term while the term occurs only in a limited number of other documents. In Information Retrieval a ranking metric that attempts to capture this intuition is [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25). The metric has proven to be one of the most successful ranking functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In one of its many versions it is computed as follows:\n",
    "\n",
    "$$score(q_1, q_2, \\ldots, q_n) = \\sum^n_{i=1} IDF(q_i) \\cdot \\frac{df(q_i, D) \\cdot (k_1 + 1)}{df(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\textrm{avgdl}})}$$\n",
    "\n",
    "where $Q$ represents a query and $df(q_i, D)$ is the frequency of the i'th term in $Q$ in document $D$. $|D|$ is the length of document $D$ in number of word tokens and avgdl is the average document length. The parameters $b$ and $k_1$ are commonly set to $0.75$ and $1.2$, respectively. We compute the I(nverse) D(ocument) F(requency) weight using:\n",
    "\n",
    "$$IDF(q_i) = \\log \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}$$\n",
    "\n",
    "where $N$ is the number of documents in the corpus and $n(q_i)$ the number of documents that contain $q_i$.\n",
    "\n",
    "We will implement this formula in our `score` method. Before we do that, we will first make a list of all the information we need to compute the formula:\n",
    "\n",
    "1. the frequency of a term $q_i$ in document $D$;\n",
    "2. the length of document $D$;\n",
    "3. the average length of all documents in the collection;\n",
    "4. the IDF weight of a term $q_i$.\n",
    "\n",
    "Our current implementation already stores the document frequency of each term in each document (1). Thus, we only need to write code to extract the last three items: (2) the length of each document, (3) the average document length and (4) the IDF weight of each unique term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class IRSystem:    \n",
    "    def __init__(self):\n",
    "        \"Initialize an IR Sytem.\"\n",
    "        self.tdf = defaultdict(Counter)\n",
    "        self.lengths = Counter()\n",
    "        self.doc_ids = []\n",
    "                \n",
    "    def index_document(self, doc_id, words):\n",
    "        \"Add a new unindexed document to the system.\"\n",
    "        self.doc_ids.append(doc_id)\n",
    "        # insert your code here\n",
    "        for word in words:\n",
    "            self.tdf[word][doc_id] += 1\n",
    "            self.lengths[doc_id] += 1\n",
    "\n",
    "    def index_collection(self, filenames):\n",
    "        \"Index a collection of documents.\"\n",
    "        for filename in filenames:\n",
    "            self.index_document(os.path.basename(filename), \n",
    "                                tokenize(open(filename).read()))\n",
    "            \n",
    "    def score(self, doc_id, *terms):\n",
    "        \"Score a document for a particular query using the sum of the term frequencies.\"\n",
    "        return sum(self.tdf[term][doc_id] for term in terms)\n",
    "            \n",
    "    def query(self, *terms, n=10):\n",
    "        \"\"\"Query the system for documents in which all terms occur. Returns\n",
    "        the top n matching documents.\"\"\"\n",
    "        scores = {doc_id: self.score(doc_id, *terms) for doc_id in self.doc_ids}\n",
    "        return sorted(scores, key=scores.get, reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "s = IRSystem()\n",
    "s.index_collection(glob.glob(path))\n",
    "\n",
    "print(s.lengths['Dawn 10892.txt'] == 192299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we have obtained the document length for each document, computing the average document length is trivial. In this exercise we will focus on the IDF weights. To compute the IDF weight for a particular term $q_i$ we need to know two things:\n",
    "\n",
    "1. how many documents $N$ there are in our collection;\n",
    "2. in how many documents that term occurs: $n(q_i)$.\n",
    "\n",
    "We will implement a helper method called `_document_frequency`. It should return a dictionary in which we store for each term the number of documents in which that term occurs. You will also need to adapt the `index_document` method in such a way that the variable `N` represents the number of documents that have been indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class IRSystem:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"Initialize an IR Sytem.\"\n",
    "        self.tdf = defaultdict(Counter)\n",
    "        self.lengths = Counter()\n",
    "        self.doc_ids = []\n",
    "        self.N = 0\n",
    "                \n",
    "    def index_document(self, doc_id, words):\n",
    "        \"Add a new unindexed document to the system.\"\n",
    "        # insert you code here\n",
    "        self.N += 1\n",
    "        self.doc_ids.append(doc_id)\n",
    "        for word in words:\n",
    "            self.tdf[word][doc_id] += 1\n",
    "            self.lengths[doc_id] += 1\n",
    "    \n",
    "    def index_collection(self, filenames):\n",
    "        \"Index a collection of documents.\"\n",
    "        for filename in filenames:\n",
    "            self.index_document(os.path.basename(filename), \n",
    "                                tokenize(open(filename).read()))\n",
    "\n",
    "    def _document_frequency(self):\n",
    "        \"Return the document frequency for each term in self.tdf.\"\n",
    "        # insert your code here\n",
    "        return {term: len(documents) for term, documents in self.tdf.items()}\n",
    "    \n",
    "    def score(self, doc_id, *terms):\n",
    "        \"Score a document for a particular query using the sum of the term frequencies.\"\n",
    "        return sum(self.tdf[term][doc_id] for term in terms)\n",
    "            \n",
    "    def query(self, *terms, n=10):\n",
    "        \"\"\"Query the system for documents in which all terms occur. Returns\n",
    "        the top n matching documents.\"\"\"\n",
    "        scores = {doc_id: self.score(doc_id, *terms) for doc_id in self.doc_ids}\n",
    "        return sorted(scores, key=scores.get, reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "s = IRSystem()\n",
    "s.index_collection(glob.glob(path))\n",
    "\n",
    "print(s._document_frequency()['children'] == 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----\n",
    "Bonus Material\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now have all the ingredients to compute the Okapi BM25 score. Lets put everything together and implement a complete version of our IR system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from math import log\n",
    "\n",
    "class IRSystem:\n",
    "    \"\"\"A very simple Information Retrieval System. The constructor \n",
    "    s = IRSystem() builds an empty system. Next, index several documents \n",
    "    with s.index_document(text, url). Then ask queries with \n",
    "    s.query('term1', 'term2', n=10) to retrieve the top n \n",
    "    matching documents.\"\"\"\n",
    "    \n",
    "    def __init__(self, b=0.75, k1=1.2):\n",
    "        \"Initialize an IR Sytem.\"\n",
    "        self.N = 0\n",
    "        self.lengths = Counter()\n",
    "        self.tdf = defaultdict(Counter)\n",
    "        self.doc_ids = []\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "        self._all_set = False\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '<IRSystem(b={self.b}, k1={self.k1}, N={self.N})>'.format(self=self)\n",
    "        \n",
    "    def index_document(self, doc_id, words):\n",
    "        \"Add a new unindexed document to the system.\"\n",
    "        self.N += 1\n",
    "        self.doc_ids.append(doc_id)\n",
    "        for word in words:\n",
    "            self.tdf[word][doc_id] += 1\n",
    "            self.lengths[doc_id] += 1\n",
    "        self._all_set = False\n",
    "        \n",
    "    def index_collection(self, filenames):\n",
    "        \"Index a collection of documents.\"\n",
    "        for filename in filenames:\n",
    "            self.index_document(os.path.basename(filename), \n",
    "                                tokenize(open(filename).read()))\n",
    "    \n",
    "    def _document_frequency(self):\n",
    "        \"Return the document frequency for each term in self.tdf.\"\n",
    "        return {term: len(documents) for term, documents in self.tdf.items()}\n",
    "    \n",
    "    def score(self, doc_id, *query):\n",
    "        \"Score a document for a particular query using Okapi BM25.\"\n",
    "        score = 0\n",
    "        length = self.lengths[doc_id]\n",
    "        for term in query:\n",
    "            tf = self.tdf[term][doc_id]\n",
    "            df = self.df.get(term, 0)\n",
    "            idf = log((self.N - df + 0.5) / (df + 0.5))\n",
    "            score += (idf * (tf * (self.k1 + 1)) / \n",
    "                          (tf + self.k1 * (1 - self.b + (self.b * length / self.avg_len))))\n",
    "        return score\n",
    "    \n",
    "    def query(self, *query, n=10):\n",
    "        \"\"\"Query an indexed collection. Returns a ranked list of doc ID's sorted by\n",
    "        the computation of Okapi BM25.\"\"\"\n",
    "        if not self._all_set:\n",
    "            self.df = self._document_frequency()\n",
    "            self.avg_len = sum(self.lengths.values()) / self.N\n",
    "            self._all_set = True\n",
    "            \n",
    "        scores = {doc_id: self.score(doc_id, *query) for doc_id in self.doc_ids}\n",
    "        return sorted(scores.items(), key=lambda i: i[1], reverse=True)[:n]\n",
    "    \n",
    "    def present(self, results):\n",
    "        \"Present the query results as a list.\"\n",
    "        for doc_id, score in results:\n",
    "            print(\"%5.2f | %s\" % (100 * score, doc_id))\n",
    "            \n",
    "    def present_results(self, *query):\n",
    "        \"Query the collection and present the results.\"\n",
    "        return self.present(self.query(*query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have taken the liberty to make some minor adjustments to the class and add some methods to present the results of a query. The method `score` implements Okapi BM25 quite straightforwardly. Note that I added a boolean attribute called `_all_set` to the class. This attribute tells the system whether all the pieces of information to compute the Okapi BM25 scores have been collected. In the method `query` we ask whether all is set. If not, we first compute the document frequencies of each unique term in our collection as well as the average document length. The reason I do not compute this earlier is that after a document has been added to the index, we need to recompute all these values (note that we set `_all_set` in `index_document` to `False`). Since we do not know whether a user of this class will add some more documents after having indexed a complete collection earlier, it is safer to postpone these computations. The methods `present` and `present_results` are two helper methods to more conveniently print the results of a query.\n",
    "\n",
    "Let's test out IR System!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = IRSystem()\n",
    "s.index_collection(glob.glob('data/haggard/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.present_results(\"regeneration\", \"pharao\", \"odds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the biggest advantage of Object Oriented Programming is the ability to subclass objects. You could, for example, make a specialized IRSystem for searching through particular directories on your own laptop. In this final exercise you will implement a simple web searcher. This searcher can be initialized with a number of URLs of web pages. The searcher downloads these pages, strips all HTML markup and indexes the raw text. The searcher can then be used to query for particular web pages. \n",
    "\n",
    "Our implementation starts with a function to retrieve a webpage given a URL. The module [urllib.request](https://docs.python.org/3/library/urllib.request.html#module-urllib.request) in Python's standard library, defines a number of functions and classes to open and read URLs. The function `urlopen` opens a `HTTPResponse` object, which has a method called `read`. Upon calling the `read` method, the complete webpage will be downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "response = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Albert_einstein\")\n",
    "response.read()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transcendent [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/) is a package to parse and create HTML (and a lot more). We will use this package to read the web pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Albert_einstein\")\n",
    "page = BeautifulSoup(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the method `get_text` on this web page to obtain a textual representation of the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = page.get_text()\n",
    "print(text[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a**) Implement the function `fetch_page`. It takes as input a URL and returns a textual representation of the corresponding web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_page(url):\n",
    "    # insert your code here\n",
    "    response = urllib.request.urlopen(url)\n",
    "    page = BeautifulSoup(response.read())\n",
    "    return page.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** We implement the searcher as a subclass of the IRSystem:\n",
    "\n",
    "    class WebSearcher(IRSystem):\n",
    "    \n",
    "Overwrite the method `index_collection`. The new method takes as input a list of URLs, fetches the textual contents for each of those URLs and adds the contents to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WebSearcher(IRSystem):\n",
    "    \n",
    "    # insert your code here\n",
    "    def index_collection(self, urls):\n",
    "        for url in urls:\n",
    "            self.index_document(url, tokenize(fetch_page(url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize your searcher and add a collection of web pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searcher = WebSearcher()\n",
    "searcher.index_collection([\"https://en.wikipedia.org/wiki/Albert_einstein\",\n",
    "                           \"http://nlp.stanford.edu/IR-book/\",\n",
    "                           \"http://www.crummy.com/software/BeautifulSoup/\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally search for documents in the index using different queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searcher.present_results(\"soup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searcher.present_results(\"retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've reached the end of the chapter. Ignore the code below, it's just here to make the page pretty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "/*\n",
       "Placeholder for custom user CSS\n",
       "\n",
       "mainly to be overridden in profile/static/custom/custom.css\n",
       "\n",
       "This will always be an empty file in IPython\n",
       "*/\n",
       "<style>\n",
       "    @import url(http://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic,700,700italic);\n",
       "\n",
       "    div.cell{\n",
       "        font-family:'roboto','helvetica','sans';\n",
       "        color:#444;\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "\n",
       "    div.text_cell_render{\n",
       "        font-family: 'roboto','helvetica','sans';\n",
       "        line-height: 145%;\n",
       "        font-size: 120%;\n",
       "        color:#444;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Menlo\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML at 0x1066c8358>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<p><small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Python Programming for the Humanities</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://fbkarsdorp.github.io/python-course\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">http://fbkarsdorp.github.io/python-course</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>. Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/fbkarsdorp/python-course\" rel=\"dct:source\">https://github.com/fbkarsdorp/python-course</a>.</small></p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
