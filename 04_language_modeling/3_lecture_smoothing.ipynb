{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Language Modeling: Smoothing\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"images/smooth.png\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Describe why and how we do \"smoothing\" in language modeling\n",
    "- Apply Laplace smoothing to language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MLE Estimate for Sentence\n",
    "------\n",
    "\n",
    "<center><img src=\"images/sentence.png\" width=\"700\"/></center>\n",
    "\n",
    "The probability of a Sentence (W) is the product of probability of each word, conditional on previous word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bi_2.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is the MLE for the probability of sentence that contains a word that has never been seen before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://media.giphy.com/media/xT5LMXexksEREWjrq0/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bi_3.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"Smoothing\": Robin Hood of probability\n",
    "-------\n",
    "\n",
    "\"Steals\" a little bit of probability from all the words in a __corpus__.\n",
    "\n",
    "Then spreads the probability around to all the words in a __vocabulary__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The result is the MLE of the unseen word will be greater than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](http://images.slideplayer.com/12/3426299/slides/slide_17.jpg)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do we want to do \"smoothing\"?\n",
    "---\n",
    "<br>\n",
    "<center><img src=\"https://www.wired.com/images_blogs/business/2010/08/OED.jpg\" width=\"700\"/></center>\n",
    "\n",
    "Languages are huge and sparse. Any given corpus only contains a limited number of all of possible words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Smoothing is used to better estimate the probability of English words appearing we have __not__ seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Laplace Smoothing<br>(You may remember Laplace from his other hit \"Probability Theory\")\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"http://learn-math.info/history/photos/Laplace_2.jpeg\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Laplace Smoothing \n",
    "-----\n",
    "\n",
    "Smoothing - redistributes probability mass from __frequently occurring words__ to __all possible words.__\n",
    "\n",
    "Laplace - adds one to the count of __each__ token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unsmoothed\n",
    "------\n",
    "<center><img src=\"images/og.png\" width=\"700\"/></center>\n",
    "c is count of word  \n",
    "N is total number of tokens, len(tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Laplace Smoothing, aka add-one smoothing\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/new.png\" width=\"700\"/></center>\n",
    "\n",
    "c is count of word  \n",
    "N is total number of tokens, len(tokens)  \n",
    "V is cardinality of tokens, len(set(tokens))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bi_4.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "UNK\n",
    "------\n",
    "\n",
    "UNK - short for \"unknown token\". Something you haven't seen in your corpora but is in the language\n",
    "\n",
    "[Dustbin category](https://en.wikipedia.org/wiki/Dustbin_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bi_5.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hey that works for NLP, but what about other discrete/categorical data?\n",
    "-----\n",
    "\n",
    "Yes!\n",
    "\n",
    "> In statistics, ... Laplace smoothing ... is a technique used to smooth categorical data.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Additive_smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Netflix used smoothing to make predictions for new countries\n",
    "-------\n",
    "\n",
    "<center><img src=\"https://i2.cdn.turner.com/money/dam/assets/160106161244-netflix-available-countries-map-780x439.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Laplace Smoothing is generally an awful idea\n",
    "------\n",
    "\n",
    "<center><img src=\"http://izquotes.com/quotes-pictures/quote-essentially-all-models-are-wrong-but-some-are-useful-george-e-p-box-212711.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Better Smoothing methods\n",
    "----\n",
    "\n",
    "- Additive smoothing -  Î± parameter is different from 1. Could be .02 or 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Good-Turing estimate\n",
    "- Jelinek-Mercer smoothing (interpolation) \n",
    "- Katz smoothing (backoff)\n",
    "- Witten-Bell smoothing\n",
    "- Absolute discounting\n",
    "- Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: NLP Lunch Tutorial: Smoothing Bill MacCartney 21 April 2005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Languages are large and sparse. To account for unseen words, steal a little probability from popular words give it to unpopular words.\n",
    "- Laplace Smoothing adds 1 to the count of all vocabulary words.\n",
    "- Additive Smoothing adds some number to category counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bigram model\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/bi_1.png\" width=\"700\"/></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
