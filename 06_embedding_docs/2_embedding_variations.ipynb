{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://i.imgflip.com/2bidnm.jpg\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By the end of this session, you should be able to:\n",
    "----\n",
    "\n",
    "- Describe how word embedding be extended:  \n",
    "    - Dependency-Based Word Embeddings\n",
    "    - doc2vec paragraphs and documents\n",
    "- Identify applications for cutting-edge algorithms of Word Mover Distance and Thought Vectors\n",
    "- Explain how to __EMBED ALL THE THINGS__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dependency-Based Word Embeddings\n",
    "---\n",
    "\n",
    "<center><img src=\"images/parse.png\" width=\"700\"/></center>\n",
    "\n",
    "An alternative to the bag-of-words approach is to define contexts based on the syntactic relations \n",
    "\n",
    "Define context by syntax (not just token distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Results\n",
    "----\n",
    "\n",
    "<center><img src=\"images/dependency_results.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__How are Bag-of-words (BoW) and depdency (deps) different?__\n",
    "\n",
    "BOW generates counties or cities in Florida (meronyms: part of the whole).\n",
    "\n",
    "Dependency generates other states \"brothers and sisters\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(cohyponyms: words that shares hyoponyms, belong to the same hypernym)\n",
    "\n",
    "[Source](http://www.aclweb.org/anthology/P14-2050.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Doc2Vec - the most powerful extension of word2vec\n",
    "----\n",
    "\n",
    "<center><img src=\"http://img5.picload.org/image/paagccr/doc2vec.png\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive doc2vec\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/vectors.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/naive_doc.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compare naive doc2vec to frequency counts\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/doc_matrix.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "word2vec\n",
    "------\n",
    "\n",
    "<center><img src=\"images/overview_word.png\" width=\"700\"/></center>\n",
    "\n",
    "Context is defined as a single word or a group of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Doc2vec (aka paragraph2vec or sentence embeddings) \n",
    "-----\n",
    "\n",
    "<center><img src=\"images/overview_paragraph.png\" width=\"700\"/></center>\n",
    "\n",
    "Context is defined as groups of words and paragrah id, every paragraph is given unique hash.\n",
    "\n",
    "Both paragraph vector and word vectors are use to predict the next word in a context. \n",
    "\n",
    "Each additional context does not have be a fixed length (because it is just a pointer!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Represented by a column in matrix D and every word is also mapped to a unique vector, represented by a column in matrix W . \n",
    "Additional parameters but the updates are sparse thus still efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have a MAJOR problem:<br> TOO MANY CRAFT BEERS! üçª\n",
    "----\n",
    "\n",
    "<center><img src=\"https://therooster.com/sites/default/files/styles/hero/public/beertaps-rooster.png?itok=gKvs_C7H\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Solution: Descri.beer using doc2vec\n",
    "---\n",
    "\n",
    "<center><img src=\"https://timebusinessblog.files.wordpress.com/2013/03/85632599-e1364519588629.jpg?w=360&h=240&crop=1\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do to make sense of 1.6M beer reviews?\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/beer_space.jpg\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Slides](http://www.slideshare.net/BenEverson/describeer-demo)  \n",
    "[Source code](https://github.com/beneverson/descri-beer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> The Sicilian gelato was extremely rich.\n",
    "\n",
    "vs.\n",
    "\n",
    "> The Italian ice-cream was very velvety.\n",
    "\n",
    "The statements reference the __same__ idea but share __no__ words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word Mover‚Äôs Distance (WMD)\n",
    "----\n",
    "\n",
    "<center><img src=\"images/wmd_illustration_1.png\" width=\"700\"/></center>\n",
    "\n",
    "Represent short text documents as a weighted point cloud of embedded words. \n",
    "\n",
    "The distance between two text documents A and B is the minimum cumulative distance that words from document A need to travel to match exactly the point cloud of document B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/emd_algo.png\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "EMD is a method to evaluate __dissimilarity__ between two multi-dimensional distributions in a feature space. \n",
    "\n",
    "EMD 'lifts' distance from individual features to full distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Word Mover‚Äôs Distance (WMD) is a special case of the [earth mover‚Äôs distance metric (EMD)](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)\n",
    "\n",
    "\n",
    "[Deep dive on EMD](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/RUBNER/emd.htm)\n",
    "\n",
    "Source: http://slideplayer.com/slide/5885504/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word Mover Distance Example\n",
    "----\n",
    "\n",
    "<center><img src=\"images/WMD_worked_example.png\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word Mover Distance Example\n",
    "----\n",
    "\n",
    "State-of-the-art k-nearest neighbors (kNN) classification accuracy but slowest metric to compute.\n",
    "\n",
    "(Another example of speed / accuracy trade-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source: From Word Embeddings To Document Distances](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)\n",
    "\n",
    "[Application to Data Science](http://tech.opentable.com/2015/08/11/navigating-themes-in-restaurant-reviews-with-word-movers-distance/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "everthing2vec\n",
    "----\n",
    "\n",
    "<center><img src=\"images/all_the_things.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notable Vectorizations\n",
    "-----\n",
    "\n",
    "- [Char2Vec](http://arxiv.org/abs/1508.06615)\n",
    "- [Word2Vec](https://papers.nips.cc/paper/5021distributedrepresentationsofwordsandphrasesandtheircompositionality.pdf) \n",
    "- [GloVe](http://wwwnlp.stanford.edu/pubs/glove.pdf)\n",
    "- [Doc2Vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
    "- [#TAGSPACE](http://emnlp2014.org/papers/pdf/EMNLP2014194.pdf)\n",
    "- [Gene2Vec](https://davidcox143.github.io/Gene2vec/) \n",
    "- [Item2Vec](https://arxiv.org/abs/1603.04259)\n",
    "- [Image2Vec](https://arxiv.org/abs/1507.08818)\n",
    "- [Video2Vec](https://www.dropbox.com/s/m99k5md8461xi0s/ICIP_Paper_Revised.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](http://datascienceassn.org/content/table-xx2vec-algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Emojis have meaning based on order and context\n",
    "------\n",
    "<center><img src=\"images/emjois.png\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "emojis2vec\n",
    "----\n",
    "\n",
    "<center><img src=\"https://s3.amazonaws.com/instagram-static/engineering-blog/emoji-hashtags/tsne_map_tight.png\" width=\"700\"/></center>\n",
    "\n",
    "[Open in new tab](https://s3.amazonaws.com/instagram-static/engineering-blog/emoji-hashtags/tsne_map_tight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Example: https://boingboing.net/2016/06/13/emojibot-uses-deep-learning-to.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What would be a business use for emjoi2vec?\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Trust & Safety\n",
    "\n",
    "üçë & üçÜ are sometime correlated with violating terms of service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image](images/https://s3.amazonaws.com/instagram-static/engineering-blog/emoji-hashtags/tsne_map_tight.png)  \n",
    "[Source](http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Embed Everything in the Space\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Airbnb Search\n",
    "------\n",
    "\n",
    "<center><img src=\"images/airbnb.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Airbnb Search\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*9p1BTgktdbZc42cR35Dd7g.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Airbnb Search\n",
    "-----\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*dOy3xKj5ts1FW_YypI1MAQ.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source:\n",
    "\n",
    "- https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3\n",
    "- https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thought Vectors\n",
    "---\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*KYLrhDHqAAdQaJiN1G4ytA.jpeg\" style=\"width: 400px;\"/>\n",
    "\n",
    "Geoffrey Hinton's, from Google, \"Top Secret\" new algorithm.\n",
    "\n",
    "Instead of embedding words or documents in vector space, embed thoughts in vector space. Their features will represent how each thought relates to other thoughts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When Google farts üí®, the rest of the world üí©\n",
    "------\n",
    "\n",
    "It hasn't been released so it is mostly speculation. Keep your eye out for it.\n",
    "\n",
    "[Thought2vec teaser](https://wtvox.com/robotics/google-is-working-on-a-new-algorithm-thought-vectors)  \n",
    "[General introduction](http://deeplearning4j.org/thoughtvectors.html)<br>\n",
    "[Skip-Thought Vectors paper](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "StarSpace: Embed All The Things!\n",
    "------\n",
    "\n",
    "Learns to represent objects of different types into a common vectorial embedding space, hence the star (‚Äò*‚Äô, wildcard). \n",
    "\n",
    "Since all \"entities\" are in the same space it is easy to compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "StarSpace Model\n",
    "------\n",
    "\n",
    "Learning entities, each of which is described by a set of discrete features (bag-of- features) coming from a fixed-length dictionary. \n",
    "\n",
    "An user entity can be described by the bag of documents, movies or items they have watched.\n",
    "\n",
    "To train our model, we need to learn to compare entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "StarSpace Loss Function\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/star_loss.png\" width=\"700\"/></center>\n",
    "(The ‚ù§Ô∏è of ML research is defining a reasonable loss function.)\n",
    "\n",
    "The generator of positive entity pairs (a, b) coming from the set E+. \n",
    "\n",
    "The generator of negative entities b<sub>i</sub><sup>-</sup> coming from the set E‚àí. \n",
    "\n",
    "We utilize a [k-negative sampling strategy](https://www.quora.com/What-is-negative-sampling)to select k such negative pairs for each batch update. \n",
    "\n",
    "We select randomly from within the set of entities that can appear in the second argument of the similarity function (e.g., for text labeling tasks a are documents and b are labels, so we sample b‚àí from the set of labels). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Empirical results on a number of tasks show that StarSpace is highly competitive with existing methods, whilst also being generally applicable to new cases where those methods are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "StarSpace Applications\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://pbs.twimg.com/media/DJsNoNlXUAEes0g.png:large\" width=\"700\"/></center>\n",
    "\n",
    "- Learning word, sentence or document level embeddings \n",
    "- Labeling tasks such as text classification\n",
    "- Collaborative filtering-based or content-based recommendation\n",
    "- Embedding of multi-relational graphs\n",
    "- Ranking tasks such as information retrieval/web search\n",
    "\n",
    "Wide variety of problems..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://a248.e.akamai.net/static.chegg.com/assets/site/logos/Chegg_1024x1024.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[StarSpace code](https://github.com/facebookresearch/StarSpace)  \n",
    "[StarSpace paper](https://arxiv.org/abs/1709.03856)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Possible Research Project\n",
    "-------\n",
    "\n",
    "The [StarSpace model](https://papers.nips.cc/paper/6571-exponential-family-embeddings.pdf) only works on discrete features.\n",
    "\n",
    "There is [\"old\" NIPS paper that extended word2vec to continuous features](https://papers.nips.cc/paper/6571-exponential-family-embeddings.pdf) via the exponential family.\n",
    "\n",
    "It would be a nice project to unite those threads.\n",
    "\n",
    "If anyone is interested, I'll be open to exploring this and writing a paper in August."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hashing is a Honkin' Good Idea\n",
    "------\n",
    "\n",
    "<center><img src=\"https://www.cs.cmu.edu/~adamchik/15-121/lectures/Hashing/pix/hashing0.bmp\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use ML to Learn Hashing\n",
    "------\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*yHbgRO_yieWXKZDOXamziQ.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "https://arxiv.org/abs/1712.01208\n",
    "\n",
    "https://medium.com/@juanmirocks/paper-review-2017-the-case-for-learned-index-structures-6ad657401123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use ML to Learn Semantic Hashing\n",
    "------\n",
    "\n",
    "<center><img src=\"images/semantic_hashing.png\" width=\"700\"/></center>\n",
    "\n",
    "Meaningful numbers, not just convient numbers\n",
    "\n",
    "Be flexible in defining space: floats, vectors, matrices, tensors, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: http://slideplayer.com/slide/5947543/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "---\n",
    "\n",
    "- Other semantic meanings can be captured by using dependency parsing as context.\n",
    "- Longer pieces of text can also be embedded into the same space as words (i.e., doc2vec).\n",
    "- Given the properties of word2vec (e.g., large input, straightforward training, and vector output), it can be applied to a variety of problems.\n",
    "    - Emojis\n",
    "    - Thoughts\n",
    "    - Everything\n",
    "    - `<insert your idea here>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Extra Slides\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Machine Translation\n",
    "---\n",
    "\n",
    "<center><img src=\"images/machine_translation.png\" width=\"1000\"/></center>\n",
    "\n",
    "Language translations can be thought of linear transformations: rotations and scalings of the vector space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How are Machine Translations learned?\n",
    "----\n",
    "\n",
    "The transform matrix can be learned by bootstrapping from a small sample (manually labeled), then extend to entire language.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a word embedding in both languages\n",
    "2. Manually specify pairs (typically - simple concrete nouns)\n",
    "3. Find the translation matrix\n",
    "4. Apply translation matrix across entire language \n",
    "\n",
    "Allows to more closely approximate language idioms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "GloVe: Global Vectors for Word Representation\n",
    "-----\n",
    "\n",
    "GloVe is a \"count-based\" model that learns geometrical encodings (vectors) of words from their co-occurrence information.\n",
    "\n",
    "Trains on the aggregated overall word-word co-occurrence matrix, which tabulates how frequently words co-occur with one another.\n",
    "\n",
    "\n",
    "\n",
    "Learns the __ratio of the co-occurrence probabilities of two words__ (word2vec learns their co-occurrence probabilities themselves)\n",
    "\n",
    "This ratio encodes important information as vector differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "GloVe: Global Vectors for Word Representation\n",
    "------\n",
    "\n",
    "<center><img src=\"https://nlp.stanford.edu/projects/glove/images/table.png\" width=\"700\"/></center>\n",
    "\n",
    "For example, __ice__ co-occurs more frequently with __solid__ than it does with __gas__.\n",
    "\n",
    "Whereas __steam__ co-occurs more frequently with __gas_- than it does with solid. \n",
    "\n",
    "Both words co-occur with their shared property __water__ frequently.\n",
    "\n",
    "Both co-occur with the unrelated word __fashion__ infrequently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "[Orginal Paper](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "GloVe: Global Vectors for Word Representation\n",
    "----\n",
    "<center><img src=\"http://building-babylon.net/wp-content/uploads/2016/02/glove-matrix-factorisation-5.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/glove_algo.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Reference](http://text2vec.org/glove.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
