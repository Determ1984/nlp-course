{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes (NB) for text classification\n",
    "----\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"http://img.scoop.it/_VP0qLV-jYbp2cFF4H4k4jl72eJkfbmt4t8yenImKBVvK0kTmF0xjctABnaLJIm9\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By the end of this session, you should be able to:\n",
    "---\n",
    "\n",
    "- Apply Naive Bayes formula for text classification\n",
    "- Identify situations when __not__ to apply Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "----\n",
    "\n",
    "What is classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predict a group from a discrete set of groups.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Puppy or Lion?\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/prediction.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Supervised Machine Learning\n",
    "------\n",
    "\n",
    "<center><img src=\"images/pipeline.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/modeling_for_cash.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://image.slidesharecdn.com/bathcamp-110803035232-phpapp01/95/adding-machine-learning-to-a-web-app-10-728.jpg?cb=1312344104\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Peter Norvig on Machine Learning at Google in 2011\n",
    "-----\n",
    "\n",
    "> And it was fun looking at the comments, because you’d see things like ‘well, I’m throwing in this naive Bayes now, but I’m gonna come back and fix it it up and come up with something better later.’ \n",
    "> \n",
    "> And the comment would be from 2006. [laughter] And I think what that says is, when you have enough data, sometimes, you don’t have to be too clever about coming up with the best algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does Bayes Rule apply to document classfication?\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://www.saedsayad.com/images/Bayes_rule.png\" width=\"700\"/></center>\n",
    "\n",
    "Features are conditionally independent given the class, each feature can predict multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes Limitation\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"http://www.azquotes.com/picture-quotes/quote-well-it-may-be-all-right-in-practice-but-it-will-never-work-in-theory-warren-buffett-86-75-88.jpg\" width=\"700\"/></center>\n",
    "\n",
    "__Despite feature independence assumption__, the efficiency of NB has witnessed its widespread development in real world applications including:\n",
    "\n",
    "- Email filtering\n",
    "- Fraud detection\n",
    "- Medical diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where does NB NOT work?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/anti_bayes.jpg\" width=\"700\"/></center>\n",
    "\n",
    "Feature correllation is needed to separate groups.\n",
    "\n",
    "These groups are separable but a NB classifer will be at chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.youtube.com/watch?v=feBKiAdhYkc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "---\n",
    "\n",
    "- Classification is supervised learning to predict a discrete labeled data.\n",
    "- One goal of machine learning is effectively and efficiently learn these representations of data and models.\n",
    "- Naive Bayes (NB) is among the simplest (and most effective) machine learning classifiers. \n",
    "- Naive Bayes (NB) makes the strong assumption that all features are conditionally independent given the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MACHINE LEARNING = REPRESENTATION + EVALUATION + OPTIMIZATION\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: [A Few Useful Things to Know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Representation\n",
    "-----\n",
    "\n",
    "The features and models be represented in a computer can handle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If states of the features and models can be reached, they cannot be learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are examples of Representation?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Naive Bayes\n",
    "- Support Vector Machine\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Evaluation\n",
    "----\n",
    "\n",
    "Distinguish better classifiers and worse classifers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Examples: \n",
    "\n",
    "- Loss or cost functions for classifer training\n",
    "- Metrics for classifer prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are examples of Loss Functions?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 0 - 1\n",
    "- linear\n",
    "- squared\n",
    "- hinge\n",
    "- log-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimization\n",
    "----\n",
    "\n",
    "Effectively and efficiently search among the classifiers for the highest-scoring one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Normal equations\n",
    "\n",
    "- Stochastic gradient descent (SGD)\n",
    "\n",
    "- Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes for Text classification\n",
    "----\n",
    "\n",
    "- Representation:\n",
    "    - Data is a bag-of-words (counts or tf-idf)\n",
    "    - Classifer is a hyperplane "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Evaluation: A confusion matrix of predicted by observed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Optimization: Calculating class probabilities and conditional probabilities on training data and then values of these probabilities are used to classify new observations. Predict the most probable class given a test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Improving on bag-of-words\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"images/tf-idf.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/flat.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "What is the MLE for a word that's never been seen before (without smoothing?) \n",
    "---\n",
    "\n",
    "![](images/mle_wo_smoothing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Smoothing for Naive Bayes\n",
    "---\n",
    "\n",
    "<center><img src=\"images/mle_w_smoothing.png\" width=\"700\"/></center>\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
