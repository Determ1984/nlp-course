Word2Vec
----

__Introductory__:

- [The amazing power of word vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)

__Required__:

- Read [Deep or Shallow, NLP Is Breaking Out](http://dl.acm.org/citation.cfm?id=2874915)  
- Read: [Word embeddings](http://sebastianruder.com/word-embeddings-1/)
- Watch 
    - [Ali Ghodsi's lecture on word2vec part 1](https://www.youtube.com/watch?v=TsEGsdVJjuA) (1.25 hours)
    - Slides are in readings folder

__Optional__:

- Watch [Ali Ghodsi's lecture on word2vec word2vec part 2](https://www.youtube.com/watch?v=nuirUEmbaJU)
- Read Section 1 of [Continuous space language models](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester2_2009_10/sdarticle.pdf)

- Read [word2vec Parameter Learning Explained](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)
- [Download pretrained vectors](http://www.marekrei.com/projects/vectorsets/)

__Challenge__:

- [Swivel: Improving Embeddings by Noticing What's Missing](https://arxiv.org/abs/1602.02215)
- [Extend word2vec to graphs](https://blog.acolyer.org/2017/09/15/struc2vec-learning-node-representations-from-structural-identity/)
- [Text embedding models contain bias](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html)